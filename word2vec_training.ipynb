{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How bias changes with training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook contents:\n",
    "1. Bias metrics in the existing literature\n",
    "2. Training of word2vec on different training datasets (of varying sizes and sources)\n",
    "3. Visualizations and computations of bias metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bias metrics in the existing literature\n",
    "\n",
    "There are 3 prominent metrics in the literature (among others): projection of word embeddings along a gender direction (Bolukbasi, modified by Nissim), WEAT, and WEFAT (Caliskan, Bryson, Narayanan). Below we summarize these papers.\n",
    "\n",
    "#### 1. Bolukbasi et al. (NIPS 2016): \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\" https://arxiv.org/abs/1607.06520\n",
    "\n",
    "#### 2. Nissim, van Noord, and van der Goot (2019): \"Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor\" https://arxiv.org/abs/1905.09866\n",
    "\n",
    "#### 3. Caliskan, Bryson, and Narayanan (Science 2017): \"Semantics derived automatically from language corpora contain human-like biases\" https://purehost.bath.ac.uk/ws/portalfiles/portal/168480066/CaliskanEtAl_authors_full.pdf\n",
    "\n",
    "* IAT is one technique used (outside of word embeddings) to measure implicit human biases:\n",
    "    * \"Implicit Association Test\": assessment (on humans) introduced by Greenwald et al., where a word is categorized into one of two categories, and faster reaction time is considered as more deeply rooted association\n",
    "* Introduces new statistical test **WEAT** (Word Embedding Association Test) to measure biases in word embeddings\n",
    "    * analogous to the IAT: interpretation is \"how separated the two distributions (of associations between target and attribute) are\"\n",
    "    * inputs:\n",
    "        * 2 sets of target words:\n",
    "            * X. programmer/engineer/scientist/...\n",
    "            * Y. nurse/teacher/librarian/...) \n",
    "        * 2 sets of attribute words:\n",
    "            * A. man/male/...\n",
    "            * B. woman/female/...):\n",
    "    * test statistic:\n",
    "        * intuitively, difference between association of two sets of target words, with attributes\n",
    "        * $s(X, Y, A, B) = \\sum_{x \\in X} s(x, A, B) - \\sum_{y \\in Y} s(y, A, B)$\n",
    "        * $s(w, A, B) = mean_{a \\in A} cos(w, a) - mean_{b \\in B} cos(w, b)$\n",
    "    * p-value of permutation test (permuting target words)\n",
    "        * $Pr_i[s(X_i, Y_i, A, B) > s(X, Y, A, B)]$\n",
    "        * effect size: $\\frac{mean_{x \\in X} s(x, A, B) - mean_{y \\in Y} s(y, A, B)}{sd_{w \\in X \\bigcup Y} s(w, A, B)}$\n",
    "    * Obtains similar results to original finding in Greenwald et al.\n",
    "* Introduces **WEFAT** (Word Embedding Factual Association Test):\n",
    "    * instead of using target word embeddings, use real-valued factual property, e.g. % female workers in occupation)\n",
    "    * difference in avg. cos similarity (between attribute A and target property, vs. attribute B and target property), divided by standard deviation of cos similarity (across each combination of attribute - target property) \n",
    "    * high correlation between % of women in different occupations, vs. strength of association of word vector w/ female gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. word2vec model training\n",
    "\n",
    "Dataset grid: 20 datasets\n",
    "\n",
    "Training size:\n",
    "* 25%\n",
    "* 50%\n",
    "* 75%\n",
    "* 100%\n",
    "\n",
    "Source:\n",
    "* TweetEval: labeled tweet dataset (e.g. sentiment, hate, emotion): https://huggingface.co/datasets/tweet_eval\n",
    "* Reddit: unlabeled reddit post dataset: https://huggingface.co/datasets/reddit\n",
    "* CNN/DailyMail: news article text with highlights: https://huggingface.co/datasets/cnn_dailymail\n",
    "* Pretrained historical word vectors: pre-trained word vectors trained on historical books from various decades from 1880s - 1990s: https://nlp.stanford.edu/projects/histwords/\n",
    "* The New York Times Annotated Corpus: articles with metadata from 1987-2007: https://catalog.ldc.upenn.edu/LDC2008T19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"cnn_dailymail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/morrisreeves/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df26f4df76f4c32a52fd08a55924f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local embeddings with shape: (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for random subsetting of data (to reduce training set to make training feasible)\n",
    "random.seed(1)\n",
    "\n",
    "if DATA == \"tweet_eval\":\n",
    "    dataset = load_dataset(DATA, \"hate\")\n",
    "    X_train = dataset['train']['text']\n",
    "    X_train = [i.replace('@user', '') for i in X_train] #strip @ mentions\n",
    "elif DATA == \"reddit\":\n",
    "    dataset = load_dataset(DATA)\n",
    "    X_train = dataset['train']['text']\n",
    "elif DATA == \"cnn_dailymail\":\n",
    "    dataset = load_dataset(DATA, '3.0.0')\n",
    "    X_train = dataset['train']['article']\n",
    "    X_train = random.sample(X_train, 5000)\n",
    "\n",
    "EMBEDDINGS_FILE = f'{DATA}_embeddings.npy'\n",
    "\n",
    "# Load trained embeddings, if exists\n",
    "if exists(EMBEDDINGS_FILE):\n",
    "    learned_embeddings = np.load(EMBEDDINGS_FILE)\n",
    "    print(f\"Loaded local embeddings with shape: {learned_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "import re\n",
    "\n",
    "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~—“”'\n",
    "\n",
    "def remove_punc(text: str, punc: str) -> str:\n",
    "    return text.translate({ord(p):'' for p in punc})\n",
    "\n",
    "def split_on_whitespace(text: str) -> List[str]:\n",
    "    if text: return re.split('\\s+', text)\n",
    "    else: return []\n",
    "\n",
    "def tokenize(text: str, punc: str) -> List[str]:\n",
    "    return [i for i in split_on_whitespace(remove_punc(text, punc).lower()) if i]\n",
    "\n",
    "def load_corpus(texts: List[str], punc: str) -> List[List[str]]:\n",
    "    tokenized_docs = []\n",
    "    for t in texts:\n",
    "        tokenized_docs.append(tokenize(t, punc))\n",
    "    return tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['colombo', 'sri', 'lanka', 'cnn', 'a', 'sri', 'lanka', 'politician', 'ended', 'his']\n"
     ]
    }
   ],
   "source": [
    "# list of lists (of tokens): each element of the list is a document (e.g. tweet, news article, etc.)\n",
    "full_tokenized = load_corpus(X_train, punc)\n",
    "print(full_tokenized[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim package implementation of CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=full_tokenized, size = 100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8530028462409973),\n",
       " ('boy', 0.771740198135376),\n",
       " ('girl', 0.7681539058685303),\n",
       " ('teenager', 0.7364521026611328),\n",
       " ('himself', 0.7291650772094727),\n",
       " ('driver', 0.7270429134368896),\n",
       " ('person', 0.693087637424469),\n",
       " ('doctor', 0.6928565502166748),\n",
       " ('soldier', 0.688307523727417),\n",
       " ('dog', 0.6637560129165649)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sense checking the trained word vectors using gensim's built-in methods...\n",
    "model.wv.most_similar(positive=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuesday', 0.9584875106811523),\n",
       " ('wednesday', 0.9520950317382812),\n",
       " ('thursday', 0.9432012438774109),\n",
       " ('friday', 0.9404246807098389),\n",
       " ('sunday', 0.8951826095581055),\n",
       " ('saturday', 0.8904947638511658),\n",
       " ('yesterday', 0.6942245364189148),\n",
       " ('board', 0.6639047265052795),\n",
       " ('arraigned', 0.6430783867835999),\n",
       " ('bail', 0.6226698160171509)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"monday\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual implementation of CBOW:\n",
    "(Borrowed from AC295 NLP assignment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor, LongTensor, BoolTensor, FloatTensor\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import Module, Embedding, Linear, init, Sequential, LogSoftmax, NLLLoss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "PADDING_IDX = -1\n",
    "CONTEXT_SIZE = 5\n",
    "VECTOR_SIZE = 100\n",
    "collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=PADDING_IDX)\n",
    "\n",
    "def create_vocab(tokenized_texts: List[List[str]]) -> Dict[str, int]:\n",
    "    all_tokens = [token for tweet in tokenized_texts for token in tweet]\n",
    "    vocab = {token:idx for idx,(token,count) in enumerate(Counter(all_tokens).most_common())}\n",
    "    return vocab\n",
    "\n",
    "# Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts: List[List[str]], vocab: Dict[str, int], vocab_size: int):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokenized_texts)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        return Tensor([self.vocab[token] for token in self.tokenized_texts[idx]\n",
    "                       if ((token in self.vocab) and (self.vocab[token] < self.vocab_size))]).long()\n",
    "\n",
    "# Helper function \n",
    "def split_data(batch: LongTensor, context_size: int) -> Tuple[LongTensor, LongTensor]:\n",
    "    # Extract a contexts tensor and targets tensor\n",
    "    contexts = batch.unfold(dimension = 1, \n",
    "                            size = 2*context_size + 1,\n",
    "                            step = 1)\n",
    "    targets = contexts[::,::,context_size]\n",
    "    mask = list(range(2*context_size + 1))\n",
    "    mask.remove(context_size) # Remove middle index, so mask selects everything but the middle\n",
    "    contexts = contexts[::,::,mask]\n",
    "    return (contexts, targets) #(B x C x 2N) and (B x C)\n",
    "    \n",
    "# Helper function \n",
    "def flatten_tensors(context: LongTensor, target: LongTensor, padding_idx: int) -> Tuple[LongTensor, LongTensor]:\n",
    "    # Return:\n",
    "    # flattened 2D contexts tensor with dimensions M × 2N\n",
    "    # flattened 1D targets tensor with dimension M\n",
    "    # where M is the number of (non-pad) words in the batch with valid context windows\n",
    "    flat_contexts = context.reshape(-1, context.shape[2])\n",
    "    flat_targets = target.reshape(-1,)\n",
    "    mask = (flat_contexts.min(axis=1).values != padding_idx)&(flat_targets != padding_idx)\n",
    "    return (flat_contexts[mask], flat_targets[mask])\n",
    "\n",
    "# Model class\n",
    "class CBOW(Module):\n",
    "    def __init__(self, context_size: int, vocab_size: int, vector_size: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        # (a) save instance variables \n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_size = vector_size\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        # (b) define the layers/architecture of the network based on instance variables\n",
    "        self.embedding = Embedding(num_embeddings = self.vocab_size,\n",
    "                                   embedding_dim = self.vector_size,\n",
    "                                   padding_idx = self.padding_idx)\n",
    "        self.linear = Linear(in_features = self.vector_size,\n",
    "                             out_features = self.vocab_size,\n",
    "                             bias = False)\n",
    "        self.logsoftmax = LogSoftmax(dim = 1)\n",
    "        \n",
    "        # (c) initialize the layers of the network\n",
    "        init.zeros_(self.embedding.weight)\n",
    "        init.uniform_(self.linear.weight, a=-1, b=1)\n",
    "    \n",
    "    def forward(self, batch: LongTensor) -> Tuple[FloatTensor, LongTensor]:\n",
    "        # input batch: B x T (batch size x sequence length)\n",
    "        # returns: (preds, targets)\n",
    "        # PREDS: 2D FloatTensor with dimensions M x V\n",
    "            #M  is the number of (non-pad) words in the batch\n",
    "            #𝑉  is the size of the vocabulary\n",
    "        # TARGETS: 1D LongTensor with length M\n",
    "            #true identity (i.e. \"target\") of the  𝑖 -th word in the batch.\n",
    "        contexts, targets = split_data(batch, self.context_size)\n",
    "        contexts, targets = flatten_tensors(contexts, targets, self.padding_idx)\n",
    "        \n",
    "        embedded = self.embedding(contexts)\n",
    "        projected = torch.mean(embedded, axis = 1)\n",
    "        output = self.linear(projected)\n",
    "        preds = self.logsoftmax(output)\n",
    "        return (preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "N_EPOCHS = 20\n",
    "LR  = 0.001\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "full_vocab = create_vocab(full_tokenized)\n",
    "print(\"full vocab size:\", len(full_vocab))\n",
    "\n",
    "full_dataset = CustomDataset(full_tokenized, full_vocab, vocab_size=VOCAB_SIZE)\n",
    "print(\"dataset shape:\", len(full_dataset))\n",
    "\n",
    "full_dataloader = DataLoader(dataset = full_dataset,\n",
    "                             batch_size = BATCH_SIZE, # documents per batch\n",
    "                             shuffle = True, # documents used to form batch are randomly selected each time\n",
    "                             collate_fn = collate_fn,\n",
    "                             drop_last = True) # ensure never less than 32 documents per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full vocab size: 105360\n",
      "dataset shape: 5000\n",
      "tensor([[  18,  797,  787,  ...,   -1,   -1,   -1],\n",
      "        [  18, 2115, 1383,  ...,   -1,   -1,   -1],\n",
      "        [ 119, 4770,   60,  ...,   -1,   -1,   -1],\n",
      "        ...,\n",
      "        [3397,  119,  518,  ...,   -1,   -1,   -1],\n",
      "        [1387,  397,   26,  ...,   -1,   -1,   -1],\n",
      "        [  18,  454,  626,  ...,   -1,   -1,   -1]])\n",
      "Dimensions of batch Tensor: torch.Size([32, 1190])\n",
      "Device: cuda\n",
      "[1,    10] loss: 8.500\n",
      "[1,    20] loss: 8.454\n",
      "[1,    30] loss: 8.392\n",
      "[1,    40] loss: 8.313\n",
      "[1,    50] loss: 8.207\n",
      "[1,    60] loss: 8.072\n",
      "[1,    70] loss: 7.912\n",
      "[1,    80] loss: 7.732\n",
      "[1,    90] loss: 7.540\n",
      "[1,   100] loss: 7.346\n",
      "[1,   110] loss: 7.160\n",
      "[1,   120] loss: 6.975\n",
      "[1,   130] loss: 6.836\n",
      "[1,   140] loss: 6.759\n",
      "[1,   150] loss: 6.690\n",
      "[2,    10] loss: 6.633\n",
      "[2,    20] loss: 6.608\n",
      "[2,    30] loss: 6.608\n",
      "[2,    40] loss: 6.594\n",
      "[2,    50] loss: 6.586\n",
      "[2,    60] loss: 6.558\n",
      "[2,    70] loss: 6.548\n",
      "[2,    80] loss: 6.565\n",
      "[2,    90] loss: 6.548\n",
      "[2,   100] loss: 6.531\n",
      "[2,   110] loss: 6.553\n",
      "[2,   120] loss: 6.513\n",
      "[2,   130] loss: 6.542\n",
      "[2,   140] loss: 6.514\n",
      "[2,   150] loss: 6.509\n",
      "[3,    10] loss: 6.502\n",
      "[3,    20] loss: 6.498\n",
      "[3,    30] loss: 6.492\n",
      "[3,    40] loss: 6.480\n",
      "[3,    50] loss: 6.481\n",
      "[3,    60] loss: 6.496\n",
      "[3,    70] loss: 6.488\n",
      "[3,    80] loss: 6.463\n",
      "[3,    90] loss: 6.457\n",
      "[3,   100] loss: 6.475\n",
      "[3,   110] loss: 6.463\n",
      "[3,   120] loss: 6.450\n",
      "[3,   130] loss: 6.454\n",
      "[3,   140] loss: 6.458\n",
      "[3,   150] loss: 6.453\n",
      "[4,    10] loss: 6.422\n",
      "[4,    20] loss: 6.429\n",
      "[4,    30] loss: 6.423\n",
      "[4,    40] loss: 6.416\n",
      "[4,    50] loss: 6.396\n",
      "[4,    60] loss: 6.398\n",
      "[4,    70] loss: 6.404\n",
      "[4,    80] loss: 6.388\n",
      "[4,    90] loss: 6.408\n",
      "[4,   100] loss: 6.396\n",
      "[4,   110] loss: 6.392\n",
      "[4,   120] loss: 6.390\n",
      "[4,   130] loss: 6.384\n",
      "[4,   140] loss: 6.383\n",
      "[4,   150] loss: 6.385\n",
      "[5,    10] loss: 6.351\n",
      "[5,    20] loss: 6.352\n",
      "[5,    30] loss: 6.342\n",
      "[5,    40] loss: 6.344\n",
      "[5,    50] loss: 6.340\n",
      "[5,    60] loss: 6.336\n",
      "[5,    70] loss: 6.322\n",
      "[5,    80] loss: 6.321\n",
      "[5,    90] loss: 6.347\n",
      "[5,   100] loss: 6.337\n",
      "[5,   110] loss: 6.308\n",
      "[5,   120] loss: 6.327\n",
      "[5,   130] loss: 6.311\n",
      "[5,   140] loss: 6.311\n",
      "[5,   150] loss: 6.306\n",
      "[6,    10] loss: 6.294\n",
      "[6,    20] loss: 6.275\n",
      "[6,    30] loss: 6.291\n",
      "[6,    40] loss: 6.283\n",
      "[6,    50] loss: 6.268\n",
      "[6,    60] loss: 6.260\n",
      "[6,    70] loss: 6.269\n",
      "[6,    80] loss: 6.285\n",
      "[6,    90] loss: 6.270\n",
      "[6,   100] loss: 6.255\n",
      "[6,   110] loss: 6.266\n",
      "[6,   120] loss: 6.249\n",
      "[6,   130] loss: 6.257\n",
      "[6,   140] loss: 6.244\n",
      "[6,   150] loss: 6.227\n",
      "[7,    10] loss: 6.231\n",
      "[7,    20] loss: 6.221\n",
      "[7,    30] loss: 6.221\n",
      "[7,    40] loss: 6.226\n",
      "[7,    50] loss: 6.220\n",
      "[7,    60] loss: 6.210\n",
      "[7,    70] loss: 6.209\n",
      "[7,    80] loss: 6.196\n",
      "[7,    90] loss: 6.209\n",
      "[7,   100] loss: 6.196\n",
      "[7,   110] loss: 6.211\n",
      "[7,   120] loss: 6.186\n",
      "[7,   130] loss: 6.200\n",
      "[7,   140] loss: 6.190\n",
      "[7,   150] loss: 6.206\n",
      "[8,    10] loss: 6.170\n",
      "[8,    20] loss: 6.165\n",
      "[8,    30] loss: 6.173\n",
      "[8,    40] loss: 6.177\n",
      "[8,    50] loss: 6.153\n",
      "[8,    60] loss: 6.157\n",
      "[8,    70] loss: 6.155\n",
      "[8,    80] loss: 6.151\n",
      "[8,    90] loss: 6.148\n",
      "[8,   100] loss: 6.149\n",
      "[8,   110] loss: 6.161\n",
      "[8,   120] loss: 6.135\n",
      "[8,   130] loss: 6.148\n",
      "[8,   140] loss: 6.159\n",
      "[8,   150] loss: 6.139\n",
      "[9,    10] loss: 6.135\n",
      "[9,    20] loss: 6.117\n",
      "[9,    30] loss: 6.129\n",
      "[9,    40] loss: 6.126\n",
      "[9,    50] loss: 6.127\n",
      "[9,    60] loss: 6.095\n",
      "[9,    70] loss: 6.102\n",
      "[9,    80] loss: 6.085\n",
      "[9,    90] loss: 6.091\n",
      "[9,   100] loss: 6.101\n",
      "[9,   110] loss: 6.104\n",
      "[9,   120] loss: 6.101\n",
      "[9,   130] loss: 6.103\n",
      "[9,   140] loss: 6.100\n",
      "[9,   150] loss: 6.094\n",
      "[10,    10] loss: 6.077\n",
      "[10,    20] loss: 6.069\n",
      "[10,    30] loss: 6.080\n",
      "[10,    40] loss: 6.091\n",
      "[10,    50] loss: 6.075\n",
      "[10,    60] loss: 6.059\n",
      "[10,    70] loss: 6.062\n",
      "[10,    80] loss: 6.062\n",
      "[10,    90] loss: 6.066\n",
      "[10,   100] loss: 6.061\n",
      "[10,   110] loss: 6.059\n",
      "[10,   120] loss: 6.054\n",
      "[10,   130] loss: 6.064\n",
      "[10,   140] loss: 6.038\n",
      "[10,   150] loss: 6.051\n",
      "[11,    10] loss: 6.037\n",
      "[11,    20] loss: 6.052\n",
      "[11,    30] loss: 6.038\n",
      "[11,    40] loss: 6.017\n",
      "[11,    50] loss: 6.018\n",
      "[11,    60] loss: 6.026\n",
      "[11,    70] loss: 6.029\n",
      "[11,    80] loss: 6.024\n",
      "[11,    90] loss: 6.027\n",
      "[11,   100] loss: 6.016\n",
      "[11,   110] loss: 6.012\n",
      "[11,   120] loss: 6.009\n",
      "[11,   130] loss: 6.011\n",
      "[11,   140] loss: 5.998\n",
      "[11,   150] loss: 6.030\n",
      "[12,    10] loss: 5.992\n",
      "[12,    20] loss: 5.985\n",
      "[12,    30] loss: 5.993\n",
      "[12,    40] loss: 5.980\n",
      "[12,    50] loss: 5.980\n",
      "[12,    60] loss: 5.989\n",
      "[12,    70] loss: 5.990\n",
      "[12,    80] loss: 6.003\n",
      "[12,    90] loss: 5.993\n",
      "[12,   100] loss: 5.980\n",
      "[12,   110] loss: 5.964\n",
      "[12,   120] loss: 5.977\n",
      "[12,   130] loss: 5.999\n",
      "[12,   140] loss: 5.975\n",
      "[12,   150] loss: 5.992\n",
      "[13,    10] loss: 5.964\n",
      "[13,    20] loss: 5.953\n",
      "[13,    30] loss: 5.967\n",
      "[13,    40] loss: 5.957\n",
      "[13,    50] loss: 5.961\n",
      "[13,    60] loss: 5.963\n",
      "[13,    70] loss: 5.945\n",
      "[13,    80] loss: 5.951\n",
      "[13,    90] loss: 5.939\n",
      "[13,   100] loss: 5.956\n",
      "[13,   110] loss: 5.955\n",
      "[13,   120] loss: 5.940\n",
      "[13,   130] loss: 5.944\n",
      "[13,   140] loss: 5.932\n",
      "[13,   150] loss: 5.953\n",
      "[14,    10] loss: 5.949\n",
      "[14,    20] loss: 5.924\n",
      "[14,    30] loss: 5.935\n",
      "[14,    40] loss: 5.910\n",
      "[14,    50] loss: 5.925\n",
      "[14,    60] loss: 5.944\n",
      "[14,    70] loss: 5.924\n",
      "[14,    80] loss: 5.903\n",
      "[14,    90] loss: 5.922\n",
      "[14,   100] loss: 5.895\n",
      "[14,   110] loss: 5.926\n",
      "[14,   120] loss: 5.916\n",
      "[14,   130] loss: 5.914\n",
      "[14,   140] loss: 5.913\n",
      "[14,   150] loss: 5.911\n",
      "[15,    10] loss: 5.893\n",
      "[15,    20] loss: 5.891\n",
      "[15,    30] loss: 5.900\n",
      "[15,    40] loss: 5.896\n",
      "[15,    50] loss: 5.900\n",
      "[15,    60] loss: 5.914\n",
      "[15,    70] loss: 5.892\n",
      "[15,    80] loss: 5.877\n",
      "[15,    90] loss: 5.870\n",
      "[15,   100] loss: 5.897\n",
      "[15,   110] loss: 5.885\n",
      "[15,   120] loss: 5.880\n",
      "[15,   130] loss: 5.888\n",
      "[15,   140] loss: 5.895\n",
      "[15,   150] loss: 5.883\n",
      "[16,    10] loss: 5.853\n",
      "[16,    20] loss: 5.861\n",
      "[16,    30] loss: 5.861\n",
      "[16,    40] loss: 5.859\n",
      "[16,    50] loss: 5.858\n",
      "[16,    60] loss: 5.873\n",
      "[16,    70] loss: 5.856\n",
      "[16,    80] loss: 5.867\n",
      "[16,    90] loss: 5.856\n",
      "[16,   100] loss: 5.855\n",
      "[16,   110] loss: 5.884\n",
      "[16,   120] loss: 5.872\n",
      "[16,   130] loss: 5.862\n",
      "[16,   140] loss: 5.869\n",
      "[16,   150] loss: 5.837\n",
      "[17,    10] loss: 5.833\n",
      "[17,    20] loss: 5.840\n",
      "[17,    30] loss: 5.832\n",
      "[17,    40] loss: 5.832\n",
      "[17,    50] loss: 5.842\n",
      "[17,    60] loss: 5.830\n",
      "[17,    70] loss: 5.830\n",
      "[17,    80] loss: 5.831\n",
      "[17,    90] loss: 5.840\n",
      "[17,   100] loss: 5.846\n",
      "[17,   110] loss: 5.837\n",
      "[17,   120] loss: 5.826\n",
      "[17,   130] loss: 5.850\n",
      "[17,   140] loss: 5.844\n",
      "[17,   150] loss: 5.836\n",
      "[18,    10] loss: 5.818\n",
      "[18,    20] loss: 5.809\n",
      "[18,    30] loss: 5.807\n",
      "[18,    40] loss: 5.817\n",
      "[18,    50] loss: 5.804\n",
      "[18,    60] loss: 5.800\n",
      "[18,    70] loss: 5.814\n",
      "[18,    80] loss: 5.808\n",
      "[18,    90] loss: 5.808\n",
      "[18,   100] loss: 5.803\n",
      "[18,   110] loss: 5.801\n",
      "[18,   120] loss: 5.833\n",
      "[18,   130] loss: 5.838\n",
      "[18,   140] loss: 5.803\n",
      "[18,   150] loss: 5.829\n",
      "[19,    10] loss: 5.799\n",
      "[19,    20] loss: 5.799\n",
      "[19,    30] loss: 5.796\n",
      "[19,    40] loss: 5.783\n",
      "[19,    50] loss: 5.800\n",
      "[19,    60] loss: 5.777\n",
      "[19,    70] loss: 5.795\n",
      "[19,    80] loss: 5.786\n",
      "[19,    90] loss: 5.796\n",
      "[19,   100] loss: 5.797\n",
      "[19,   110] loss: 5.790\n",
      "[19,   120] loss: 5.807\n",
      "[19,   130] loss: 5.774\n",
      "[19,   140] loss: 5.787\n",
      "[19,   150] loss: 5.771\n",
      "[20,    10] loss: 5.776\n",
      "[20,    20] loss: 5.772\n",
      "[20,    30] loss: 5.773\n",
      "[20,    40] loss: 5.787\n",
      "[20,    50] loss: 5.760\n",
      "[20,    60] loss: 5.752\n",
      "[20,    70] loss: 5.780\n",
      "[20,    80] loss: 5.769\n",
      "[20,    90] loss: 5.782\n",
      "[20,   100] loss: 5.771\n",
      "[20,   110] loss: 5.782\n",
      "[20,   120] loss: 5.778\n",
      "[20,   130] loss: 5.759\n",
      "[20,   140] loss: 5.763\n",
      "[20,   150] loss: 5.757\n",
      "Finished Training\n",
      "CPU times: user 10min 38s, sys: 6.58 s, total: 10min 45s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch = next(iter(full_dataloader))\n",
    "print(batch)\n",
    "print(f\"Dimensions of batch Tensor: {batch.shape}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Train model\n",
    "model = CBOW(context_size=CONTEXT_SIZE, vocab_size=VOCAB_SIZE, \n",
    "             vector_size=VECTOR_SIZE, padding_idx=PADDING_IDX)\n",
    "model.to(device)\n",
    "criterion = NLLLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "loss_history = []\n",
    "# Based on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(N_EPOCHS):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(full_dataloader, 0):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        preds, targets = model(batch)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            loss_history.append(running_loss / 10)\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8aac8a3a00>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding (weights): (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "np.save(EMBEDDINGS_FILE, learned_embeddings) # save\n",
    "\n",
    "learned_embeddings_dict = {i:learned_embeddings[i] for i, _ in enumerate(learned_embeddings)}\n",
    "print(f\"Shape of embedding (weights): {learned_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n",
      "\tClosest 5 words: ['woman', 'teenager', 'boy', '22yearold', 'doctor']\n",
      "\tCosine similarities:\t[0.8390858 0.7929545 0.7653596 0.7119213 0.7073126]\n",
      "==================================================\n",
      "monday\n",
      "\tClosest 5 words: ['thursday', 'wednesday', 'tuesday', 'friday', 'sunday']\n",
      "\tCosine similarities:\t[0.9317715  0.9271284  0.9188106  0.91391313 0.85849553]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(doc1, doc2, bow):\n",
    "    bow1 = bow[doc1]; bow2 = bow[doc2]\n",
    "    return np.dot(bow1, bow2)/(np.sqrt(np.dot(bow1, bow1) * np.dot(bow2, bow2)))\n",
    "\n",
    "for w in [\"man\", \"monday\"]:\n",
    "    similarities = []; word_ids = []\n",
    "    for w2 in learned_embeddings_dict:\n",
    "        if np.sum(np.dot(learned_embeddings_dict[w2], learned_embeddings_dict[w2]))==0: pass\n",
    "        else:\n",
    "            word_idx = full_dataset.vocab[w]\n",
    "            similarities.append(cosine_similarity(word_idx, w2, learned_embeddings_dict))\n",
    "            word_ids.append(w2)\n",
    "    closest_idx = np.array(word_ids)[np.argsort(similarities)[::-1][1:6]]\n",
    "    print(f\"{w}\")\n",
    "    print(f\"\\tClosest 5 words: {[list(full_dataset.vocab.keys())[i] for i in closest_idx]}\")\n",
    "    print(f\"\\tCosine similarities:\\t{np.array(similarities)[np.argsort(similarities)[::-1][1:6]]}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization of bias metrics\n",
    "\n",
    "Useful package to automatically compute bias metrics:\n",
    "* automatic computation of the above bias metrics: https://docs.responsibly.ai/notebooks/demo-word-embedding-bias.html\n",
    "\n",
    "The default pip install does not work.\n",
    "To make it work:\n",
    "* pip install gensim==3.7.1\n",
    "* git clone https://github.com/ResponsiblyAI/responsibly.git\n",
    "* cd responsibly\n",
    "* delete the \"python_requires\" and \"install_requires\" arguments of responsibly/setup.py\n",
    "* python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibly.we import calc_all_weat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_all_weat(model, filter_by='model', with_original_finding=True,\n",
    "#               with_pvalue=True, pvalue_kwargs={'method': 'approximate'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
